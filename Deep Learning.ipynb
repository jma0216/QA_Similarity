{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Reading Comprehension is tackled by different research groups, but all involve deep learning.\n",
    "This notebook is to provide resources related to this topic.\n",
    "\n",
    "To learn more about machine learning and natural language processing, here are some helpful resources:\n",
    "- [Stanford's NLP class coursework, including lecture notes/videos and assignments](http://web.stanford.edu/class/cs224n/index.html#coursework)\n",
    "- [A course in machine learning](http://ciml.info/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going more deeply into this reading comprehension problem, this [github repository](https://github.com/sebastianruder/NLP-progress/blob/master/english/question_answering.md) contains the list of current research on NLP, and the link leads specifically to research in question answering.\n",
    "\n",
    "The most popular reading comprehension problem datasets are:\n",
    "- [SQuAD (Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/): passage, question, and answer that can be extracted from the passage.\n",
    "- [NewsQA](https://github.com/sebastianruder/NLP-progress/blob/master/english/question_answering.md#newsqa): story, question, and answer\n",
    "- [CNN/Daily Mail](https://github.com/sebastianruder/NLP-progress/blob/master/english/question_answering.md#cnn--daily-mail): passage with missing words, question (sentence with a placeholder), answer which is the missing word to fill the placeholder\n",
    "- and more, but most of them are either styled like the SQuAD dataset or the Cloze-style (fill in the blank) like CNN/Daily Mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context free word embedding:\n",
    "- Word2Vec : link to article about Word2Vec [[1]](https://skymind.ai/wiki/word2vec) [[2]](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/) : its pre-trained word vectors are very commonly used as the word embedding layer (spaCy's vectors use GloVe as well)\n",
    "\n",
    "In context free word embedding, each word is individually represented, creating problems such as the word \"Bank\" as both a \"bank account\" and \"bank of river.\" \n",
    "\n",
    "\n",
    "[ELMo](https://allennlp.org/elmo): deep contextualized, character based word representation\n",
    "[BERT](): deeply bidirectional, unsupervised language representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important concepts and models to know:\n",
    "\n",
    "(the lectures in the cs224n course go through each of these topics as well)\n",
    "\n",
    "- Embedding:\n",
    "    - Context free word embedding:\n",
    "        - Word2Vec : link to article about Word2Vec [[1]](https://skymind.ai/wiki/word2vec) [[2]](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "        - [GloVe](https://nlp.stanford.edu/projects/glove/) : its pre-trained word vectors are very commonly used as the word embedding layer (spaCy's vectors use GloVe as well)\n",
    "    \n",
    "    In context free word embedding, each word is individually represented, creating problems such as the word \"Bank\" as both a \"bank account\" and \"bank of river.\" \n",
    "    - Contextualized word embedding:\n",
    "        - [ELMo](https://allennlp.org/elmo): deep contextualized, character based word representation\n",
    "        - [BERT](): deeply bidirectional, unsupervised language representation\n",
    "    \n",
    "\n",
    "- Seq2Seq\n",
    "- Encoder - Decoder:\n",
    "- [CNN](http://cs231n.github.io/convolutional-networks/)\n",
    "- RNN/LSTM\n",
    "- Self-Attention\n",
    "\n",
    "\n",
    "- [QANet](https://openreview.net/pdf?id=B14TlG-RW): convolution and self-attentions\n",
    "- [BiDAF](https://arxiv.org/pdf/1611.01603.pdf): allenAI\n",
    "- [Transformer](https://arxiv.org/pdf/1706.03762.pdf): Attention in place of LSTM/RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
