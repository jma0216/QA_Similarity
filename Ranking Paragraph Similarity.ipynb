{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity import spacy_model, rank_paragraphs, find_answer_paragraph\n",
    "from pickle_docs import *\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from squad_dataset_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am going to see how my simple similarity functions are holding up to return the file containing information relevant to the question.\n",
    "\n",
    "Then, I am going to see how good it is at returning not only the correct paragraph, but the sentence that contains the answer to the question.\n",
    "\n",
    "First, I am using the SQUAD dataset to simplify the process (don't have to create my own dataset).\n",
    "In order to use this dataset, I extract the context paragraphs and the question-answer pair, then process the paragraph and question through the spaCy model \"en_vector_web_lg\" in order to be able to use spaCy's similarity pipeline that uses vector embedding using GloVe.\n",
    "\n",
    "The list of contexts (spaCy doc object) is exported as a pickle file.\n",
    "The list of tuples (index of target context paragraph, question as a spaCy doc, answer as a string) is exported as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling docs and qas\n",
      "86821\n",
      "19035\n",
      "preprocess and pickle the dataset: 116.9895018 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "preprocess_pickle(spacy_model(), \"docs/squad contexts.pickle\", \"docs/squad qas.pickle\")\n",
    "time_passed = timer() - start\n",
    "print(\"preprocess and pickle the dataset: %s seconds\" % time_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the context and (target,question,answer) are pickled, we will no longer need to process them through spacy.\n",
    "We will simply load the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the pickled files\n",
      "89.0525707 seconds to load 19035 context paragraphs and 86821 question-answers\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "contexts, qas = preprocess_unpickle(\"docs/squad contexts.pickle\", \"docs/squad qas.pickle\")\n",
    "time_passed = timer() - start\n",
    "print(\"%s seconds to load %s context paragraphs and %s question-answers\" % (time_passed, len(contexts), len(qas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we load the lists, we can start testing.\n",
    "I will be looking at just the first 100 context paragraphs and the 1165 question-answer pairs that correspond to those 100 context paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = sorted(qas, key=lambda tup:tup[0])\n",
    "sub_qas = [qa for qa in qas if qa[0] < 100]\n",
    "sub_contexts = contexts[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity functions all work by checking the similarity between the question sentence and each of the sentences within a paragraph.\n",
    "\n",
    "In similarity.find_answer_sentence, the function compares the similarity value (1 being the most similar) created by comparing the question sentence to each question in the paragraph.\n",
    "\n",
    "In find_answer_paragraph, it compares the most similar sentence from one paragraph with the most similar sentence in another paragraph. Then, the paragraph that contains the sentence (with the highest similarity value of all the sentences in all the paragraphs) is returned as the predicted target paragraph.\n",
    "\n",
    "The code snippet below will compare the predicted target paragraph with the actual target paragraph for the 1165 question-answer pairs within the context of 100 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1165/1165 [04:36<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of question-answer pairs: 1165, number of context paragraphs: 100\n",
      "number of correct paragraph index prediction: 358\n",
      "percent correct: 0.3072961373390558\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for qa in tqdm(sub_qas):\n",
    "    predicted_target_paragraph,highest_similarity_value,predicted_target_sentence = find_answer_paragraph(sub_contexts, qa[1])\n",
    "    if predicted_target_paragraph == qa[0]:\n",
    "        correct += 1\n",
    "print(\"number of question-answer pairs: %s, number of context paragraphs: %s\" %(len(sub_qas), len(sub_contexts)))\n",
    "print(\"number of correct paragraph index prediction: %s\" %correct)\n",
    "print(\"percent correct: %s\" %(correct/len(sub_qas)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took 4:08 minutes to process 1165 sentences with 30.73% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains paragraphs from articles, meaning that many of the paragraphs are from one article.\n",
    "This makes the similarity value of paragraphs that do not contain the answer pretty high. (Maybe the paragraph does contain the answer, but the question-answer pair wasn't meant to be answered by the paragraph -- I won't be addressing these types of issues)\n",
    "There are many reasons, but to see if finding the similarity between the question and paragraph is useful at all, I decided to write a function that will store the top n number of paragraphs with the highest similarity value.\n",
    "\n",
    "We can also see where the correct target value ranked (if it was ranked in the top n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1165/1165 [04:23<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of ranking of target paragraph: [358, 143, 88, 53, 39, 29, 35, 30, 27, 23]\n",
      "number of times target paragraph was included in ranking: 825 out of 1165\n"
     ]
    }
   ],
   "source": [
    "rank_list = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "wrong = 0\n",
    "\n",
    "# qa = (target paragraph index, question, answer)\n",
    "for qa in tqdm(sub_qas):\n",
    "    # top 10\n",
    "    # most_similar = list of tuples (predicted paragraph index, max similarity score, predicted sentence index)\n",
    "    most_similar = rank_paragraphs(sub_contexts, qa[1], 10)\n",
    "    paragraph_indices = [paragraph[0] for paragraph in most_similar]\n",
    "    try:\n",
    "        # find the index of the target paragraph in the paragraph ranking\n",
    "        idx = paragraph_indices.index(qa[0])\n",
    "        rank_list[idx] += 1\n",
    "    except ValueError:\n",
    "        # if the target paragraph does not exist\n",
    "        wrong += 1\n",
    "print(\"distribution of ranking of target paragraph: %s\" %rank_list)\n",
    "print(\"number of times target paragraph was included in ranking: %s out of %s\" %((len(sub_qas)-wrong),len(sub_qas)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 358 of the predictions correctly ranked the target paragraph as the best choice.\n",
    "However, the target paragraph was included in the top 10 for 825/1165 questions (70.82%), with most of them placed in the top three (589/825 = 71.39%).\n",
    "\n",
    "This took 4:17 minutes to complete 1165 questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not actually require any machine learning, which means that it does not require any training. However, this means that there is not a model that contains some network that is searching for the answer (a span of characters).\n",
    "Instead, this code compares the question sentence with sentences in a paragraph, under the assumption that the answer to the question is most likely to be in the same sentence as the question.\n",
    "This means that it does not capture any examples such as :\n",
    "    \"What is the purpose of models such as A?\"\n",
    "    \"There are models such as A. They have B purpose.\"\n",
    "    prediction: \"There are models such as A.\"\n",
    "The GloVe pretrained vector space is created by a count-based model. This is available directly through spaCy's vector model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how accurately the similarity function calculated the target sentence of the correctly predicted target paragraph.\n",
    "\n",
    "the answer_text2idx returns a list of tuples (target_paragraph_index, question, answer, target_sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_qas = answer_text2idx(contexts, qas)\n",
    "pickle_all(idx_qas, \"docs/squad idx_qas.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_qas = sorted(idx_qas, key=lambda tup:tup[0])\n",
    "trimmed_qas = {qa for qa in idx_qas if qa[0] < 100}\n",
    "trimmed_contexts = contexts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1165/1165 [04:39<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[358, 143, 88, 53, 39, 29, 35, 30, 27, 23]\n",
      "0.7081545064377682\n",
      "340 1165\n",
      "correct sentence:  556\n"
     ]
    }
   ],
   "source": [
    "rank_list = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "wrong = 0\n",
    "par_sent_correct = 0\n",
    "for qa in tqdm(trimmed_qas):\n",
    "    # (paragraph_index, max_similarity_score, sentence_of_max)\n",
    "    most_similar = rank_paragraphs(trimmed_contexts, qa[1], 10)\n",
    "    xz = [ixx[0] for ixx in most_similar]\n",
    "    try:\n",
    "        va1 = xz.index(qa[0])\n",
    "        rank_list[va1] += 1\n",
    "        sen_idx = most_similar[va1][2]\n",
    "        tar_idx = qa[3]\n",
    "        if sen_idx == tar_idx:\n",
    "            par_sent_correct += 1\n",
    "    except ValueError:\n",
    "        wrong += 1\n",
    "print(rank_list)\n",
    "print((len(trimmed_qas) - wrong)/len(trimmed_qas))\n",
    "print(wrong, len(trimmed_qas))\n",
    "print(\"correct sentence: \", par_sent_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 825 correctly identified target paragraphs, the target sentence of 556 were also correctly identified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
